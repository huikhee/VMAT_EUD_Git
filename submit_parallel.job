#!/bin/bash
#SBATCH --partition=mpcg.p
#SBATCH --gpus-per-node=1
#SBATCH --mem=128G           
#SBATCH --time=2-0:05  
#SBATCH --job-name GnT_EPID_v1          
#SBATCH --output=GnT_EPID_v1.%j.out        
#SBATCH --error=GnT_EPID_v1.%j.err          

module load Python/3.11.3-GCCcore-13.1.0
module load PyTorch/2.1.2-foss-2023a-CUDA-12.4.0

module load CUDA/12.4.0

# Start GPU monitoring in the background
#nvidia-smi --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.total,memory.used --format=csv -l 5 > $HOME/VMAT_EUD/gpu_usage.log &

# Get the total number of GPUs on the node
#NUM_GPUS=$(nvidia-smi --query-gpu=count --format=csv,noheader,nounits)

# Iterate over each GPU and start logging in a separate file
#for GPU_ID in $(seq 0 $((NUM_GPUS - 1))); do
#    nvidia-smi --id=$GPU_ID --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.total,memory.used --format=csv -l 1 > $HOME/VMAT_EUD/gpu_${GPU_ID}_usage.log &
#done

srun python generate_and_train_EPID_v5.py

# Kill the GPU monitoring process after the training script finishes
pkill -f nvidia-smi

