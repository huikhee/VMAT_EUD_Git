#!/bin/bash
#SBATCH --mem=128G           
#SBATCH --time=2-0:05  
#SBATCH --job-name GnT_VMAT          
#SBATCH --output=GnT_VMAT.%j.out        
#SBATCH --error=GnT_VMAT.%j.err          
 
module load Python/3.11.3-GCCcore-13.1.0
module load PyTorch/2.1.2-foss-2023a-CUDA-12.4.0

module load CUDA/12.4.0

# Start GPU monitoring in the background
nvidia-smi --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.total,memory.used --format=csv -l 5 > $HOME/VMAT_EUD/gpu_usage.log &

# Get the total number of GPUs on the node
#NUM_GPUS=$(nvidia-smi --query-gpu=count --format=csv,noheader,nounits)

# Iterate over each GPU and start logging in a separate file
#for GPU_ID in $(seq 0 $((NUM_GPUS - 1))); do
#    nvidia-smi --id=$GPU_ID --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.total,memory.used --format=csv -l 1 > $HOME/VMAT_EUD/gpu_${GPU_ID}_usage.log &
#done

srun python generate_and_train_amp_parallel_coll0_embedded_64_impLoss.py

# Kill the GPU monitoring process after the training script finishes
pkill -f nvidia-smi

